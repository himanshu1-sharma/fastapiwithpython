# from langchain_core.tools import Tool
# from sqlalchemy.orm import Session
# from app.repositories.chat_memory_repository import get_user_chat_memory, save_chat_memory
# from app.schemas.chat_memory_schema import ChatMemoryCreate
# from langchain_core.messages import HumanMessage, AIMessage
# from app.core.config import settings
# from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_community.vectorstores import Chroma
# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain_core.output_parsers import StrOutputParser
# from uuid import UUID
# from langchain_community.tools import DuckDuckGoSearchRun
# import traceback
# import logging
# from datetime import datetime

# logger = logging.getLogger(__name__)

# # üß† Initialize LLM and Vector DB
# try:
#     embeddings = OpenAIEmbeddings(api_key=settings.OPENAI_API_KEY)
#     CHROMA_PATH = "app/db/chroma_storage"
#     vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)
#     retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
#     llm = ChatOpenAI(model=settings.OPENAI_MODEL, temperature=0.3, api_key=settings.OPENAI_API_KEY)
#     search_tool = DuckDuckGoSearchRun()
#     logger.info("‚úÖ All tools initialized successfully")
# except Exception as init_error:
#     logger.error(f"‚ùå Initialization error: {init_error}")
#     logger.debug(traceback.format_exc())

# # üí¨ System Prompt
# prompt = ChatPromptTemplate.from_messages([
#     (
#         "system",
#         f"""
#         Tum {settings.BOT_NAME} ho ‚Äî ek intelligent, polite aur thoda witty AI assistant 
#         jise {settings.CREATOR_NAME} ne banaya hai üòé.

#         Kuch khaas baatein mere baare mein:
#         ‚Ä¢ Mujhe {settings.CREATOR_NAME} ne banaya hai.
#         ‚Ä¢ Main Hindi-English dono me baat karta hoon.
#         ‚Ä¢ Main coding, AI, aur daily life ke sawalon ka jawab deta hoon.
#         ‚Ä¢ Thoda witty hoon, par respect ke saath.
#         ‚Ä¢ Mere paas updated knowledge hai (jitna mujhe diya gaya hai).

#         Agar koi pooche "tum kaun ho" ya "apne baare me batao", 
#         to apne style me confidently ye sab batao.

#         üß† Personality Rules:
#         - Hamesha apne aap ko "{settings.BOT_NAME}" kehkar refer karo.
#         - Agar koi pooche "tum kaun ho", "tumhara naam kya hai", "kisne banaya tumhe",
#           ya "apne baare me batao", to naturally aur friendly tone me jawab do:
#           "Mera naam {settings.BOT_NAME} hai. Mujhe {settings.CREATOR_NAME} ne banaya hai.
#           Main ek AI assistant hoon jo tumhe har problem me help karta hai ‚Äî coding se leke
#           life ke sawalon tak. üòä"
#         - Tone: mix Hindi + English (Indian conversational style).
#         - Friendly, concise aur helpful raho ‚Äî unnecessarily lamba explanation mat do.
#         - Thoda witty ban sakte ho, par kabhi rude nahi hona.
#         - Hamesha apna character maintain karo, kabhi break mat karo.
#         - Agar user technical sawal pooche to professional aur clear tone me jawab do.
#         """
#     ),
#     MessagesPlaceholder(variable_name="chat_history"),
#     ("human", "Question: {input}\n\nContext:\n{context}")
# ])

# # üîç Search Prompt Template with Current Date Context
# search_prompt = ChatPromptTemplate.from_messages([
#     (
#         "system",
#         f"""
#         Tum {settings.BOT_NAME} ho ‚Äî ek intelligent AI assistant.
        
#         IMPORTANT: Aaj ki date hai {{current_date}} (DD Month YYYY format mein).
        
#         User ne tumse real-time information maangi hai. 
#         Neeche search results diye gaye hain internet se.
        
#         CRITICAL INSTRUCTIONS:
#         1. Agar user date ya time poocha hai, to HAMESHA current date {{current_date}} use karo
#         2. Search results purane ho sakte hain - unhe ignore karo agar wo outdated hain
#         3. Current date ko priority do over search results
#         4. Apne style me jawab do - Hindi-English mix, friendly aur thoda witty
#         5. Agar search results current date ke baare mein nahi hain, to directly current date bata do
#         """
#     ),
#     ("human", "Question: {question}\n\nSearch Results:\n{search_results}\n\nAnswer in your style:")
# ])


# def get_current_datetime_info():
#     """Get current date and time information"""
#     now = datetime.now()
#     return {
#         "date": now.strftime("%d %B %Y"),  # 29 October 2025
#         "time": now.strftime("%I:%M %p"),  # 02:30 PM
#         "day": now.strftime("%A"),  # Monday
#         "iso": now.isoformat()
#     }


# def chat_with_memory_db(db: Session, user_id: UUID, question: str):
#     """
#     Sharma Ji bot with memory + retrieval + real-time search capability.
#     """
#     try:
#         logger.info(f"üìù Processing question for user {user_id}: {question}")
        
#         # Get current datetime
#         current_info = get_current_datetime_info()
#         logger.info(f"üìÖ Current date: {current_info['date']}")
        
#         # üß† Load previous messages from DB
#         try:
#             db_history = get_user_chat_memory(db, user_id)
#             messages = []
#             for item in db_history:
#                 messages.append(HumanMessage(content=item.question))
#                 messages.append(AIMessage(content=item.answer))
#             logger.info(f"‚úÖ Loaded {len(messages)} previous messages")
#         except Exception as history_error:
#             logger.error(f"‚ùå Error loading chat history: {history_error}")
#             messages = []

#         # üóìÔ∏è Direct date/time questions - don't even search
#         date_time_keywords = ["aaj ki date", "today date", "aaj ka din", "current date", "kya tarikh", 
#                               "aaj kya date", "date kya hai", "time kya hai", "aaj ka time"]
        
#         if any(keyword in question.lower() for keyword in date_time_keywords):
#             logger.info("üìÖ Direct date/time question - using system time")
            
#             # Generate response directly with current date
#             direct_answer = f"Aaj ki date hai {current_info['date']} ({current_info['day']}). " \
#                           f"Aur time hai {current_info['time']}. Kuch aur jaanna hai? üòä"
            
#             # Save to DB
#             try:
#                 chat_data = ChatMemoryCreate(
#                     user_id=user_id,
#                     question=question,
#                     answer=direct_answer
#                 )
#                 save_chat_memory(db, chat_data)
#                 logger.info("‚úÖ Chat saved to database")
#             except Exception as db_error:
#                 logger.error(f"‚ùå Database save error: {db_error}")
            
#             return {
#                 "answer": direct_answer,
#                 "context": [],
#                 "source_type": "system_time"
#             }

#         # üåç Detect if question needs real-time info (other than date/time)
#         realtime_keywords = [
#             "latest", "current", "now", "news", "weather",
#             "score", "who is", "what is", "recent", "update", "2024", "2025"
#         ]
#         needs_search = any(keyword in question.lower() for keyword in realtime_keywords)

#         # üîé Perform real-time search if needed
#         if needs_search:
#             logger.info(f"üîç Real-time search triggered for: {question}")
#             try:
#                 # Enhance search query with current date for better results
#                 enhanced_query = f"{question} {current_info['date']}"
#                 search_results = search_tool.run(enhanced_query)
#                 logger.info(f"‚úÖ Search results retrieved: {len(search_results)} chars")

#                 # Create answer using search results with current date context
#                 search_chain = search_prompt | llm | StrOutputParser()
#                 search_answer = search_chain.invoke({
#                     "question": question,
#                     "search_results": search_results,
#                     "current_date": current_info['date']
#                 })
#                 logger.info("‚úÖ Search answer generated")

#                 # Save chat to DB
#                 try:
#                     chat_data = ChatMemoryCreate(
#                         user_id=user_id,
#                         question=question,
#                         answer=search_answer
#                     )
#                     save_chat_memory(db, chat_data)
#                     logger.info("‚úÖ Chat saved to database")
#                 except Exception as db_error:
#                     logger.error(f"‚ùå Database save error: {db_error}")
#                     logger.debug(traceback.format_exc())

#                 return {
#                     "answer": search_answer,
#                     "context": [],
#                     "source_type": "search"
#                 }

#             except Exception as search_error:
#                 logger.error(f"‚ùå Search failed: {search_error}")
#                 logger.debug(traceback.format_exc())
#                 # Continue to regular flow if search fails

#         # üìö Retrieve related docs from vector DB (for non-search queries or search fallback)
#         try:
#             docs = retriever.invoke(question)
#             context = "\n\n".join([doc.page_content for doc in docs]) if docs else "No relevant context found."
#             logger.info(f"‚úÖ Retrieved {len(docs)} documents from vector DB")
#         except Exception as retriever_error:
#             logger.error(f"‚ùå Retriever error: {retriever_error}")
#             logger.debug(traceback.format_exc())
#             docs = []
#             context = "No context available."

#         # üß© Run through prompt chain
#         try:
#             chain = prompt | llm | StrOutputParser()
#             answer = chain.invoke({
#                 "input": question,
#                 "chat_history": messages,
#                 "context": context
#             })
#             logger.info("‚úÖ Answer generated successfully")
#         except Exception as chain_error:
#             logger.error(f"‚ùå Chain execution error: {chain_error}")
#             logger.debug(traceback.format_exc())
#             raise

#         # üíæ Save chat to DB
#         try:
#             chat_data = ChatMemoryCreate(
#                 user_id=user_id,
#                 question=question,
#                 answer=answer
#             )
#             save_chat_memory(db, chat_data)
#             logger.info("‚úÖ Chat saved to database")
#         except Exception as db_error:
#             logger.error(f"‚ùå Database save error: {db_error}")
#             logger.debug(traceback.format_exc())

#         return {
#             "answer": answer,
#             "context": docs,
#             "source_type": "vectorstore"
#         }

#     except Exception as e:
#         logger.error(f"‚ùå Critical error in chat_with_memory_db: {e}")
#         logger.error(f"Error type: {type(e).__name__}")
#         logger.error(traceback.format_exc())
        
#         return {
#             "answer": f"Sorry, maine ek technical error encounter kiya: {str(e)}. Please try again!",
#             "context": [],
#             "source_type": "error",
#             "error": str(e)
#         }





# tavily with real time data
# from langchain_core.tools import Tool
# from sqlalchemy.orm import Session
# from app.repositories.chat_memory_repository import get_user_chat_memory, save_chat_memory
# from app.schemas.chat_memory_schema import ChatMemoryCreate
# from langchain_core.messages import HumanMessage, AIMessage
# from app.core.config import settings
# from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_community.vectorstores import Chroma
# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain_core.output_parsers import StrOutputParser
# from tavily import TavilyClient
# from uuid import UUID
# import traceback
# import logging
# from datetime import datetime

# logger = logging.getLogger(__name__)

# # Initialize LLM, Vectorstore, and Tavily Search
# try:
#     print("üîß [INIT] Starting initialization...")
#     embeddings = OpenAIEmbeddings(api_key=settings.OPENAI_API_KEY)
#     CHROMA_PATH = "app/db/chroma_storage_new"
#     print(f"üìÅ [INIT] Chroma path: {CHROMA_PATH}")

#     vectorstore = Chroma(
#         persist_directory=CHROMA_PATH,
#         embedding_function=embeddings
#     )
#     retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
#     print("‚úÖ [INIT] Vectorstore initialized")

#     llm = ChatOpenAI(
#         model=settings.OPENAI_MODEL,
#         temperature=0.3,
#         api_key=settings.OPENAI_API_KEY
#     )
#     print(f"‚úÖ [INIT] LLM initialized: {settings.OPENAI_MODEL}")

#     tavily_client = TavilyClient(api_key=settings.TAVILY_API_KEY)
#     print("‚úÖ [INIT] Tavily Search initialized successfully")
#     logger.info("Tavily Search initialized successfully")

# except Exception as init_error:
#     print(f"‚ùå [INIT ERROR] {init_error}")
#     logger.error(f"Initialization error: {init_error}")
#     logger.debug(traceback.format_exc())
#     raise

# # System Prompt (Hinglish + Personality)
# prompt = ChatPromptTemplate.from_messages([
#     (
#         "system",
#         f"""
#         Tum {settings.BOT_NAME} ho ‚Äî ek intelligent, polite aur thoda witty AI assistant 
#         jise {settings.CREATOR_NAME} ne banaya hai.

#         Rules:
#         - Mix Hindi + English
#         - Professional yet friendly
#         - Apna character hamesha maintain karo
#         - Agar user pooche "tum kaun ho", to confidently intro do
#         """
#     ),
#     MessagesPlaceholder(variable_name="chat_history"),
#     ("human", "Question: {input}\n\nContext:\n{context}")
# ])

# # Tavily Search Prompt
# search_prompt = ChatPromptTemplate.from_messages([
#     (
#         "system",
#         f"""
#         Tum {settings.BOT_NAME} ho ‚Äî ek smart AI assistant.
#         Tumhe Tavily search se realtime data milta hai.

#         Current date: {{current_date}}.
#         Jo data mila hai, use concise aur apne tone me explain karo.
#         Agar data outdated ho to user ko warn kar dena.
#         """
#     ),
#     ("human", "Question: {question}\n\nTavily Results:\n{search_results}\n\nAnswer naturally:")
# ])


# def get_current_datetime_info():
#     """Get current date/time details"""
#     now = datetime.now()
#     info = {
#         "date": now.strftime("%d %B %Y"),
#         "time": now.strftime("%I:%M %p"),
#         "day": now.strftime("%A"),
#     }
#     print(f"üìÖ [TIME] Current info: {info}")
#     return info


# def extract_content_from_doc(doc):
#     """
#     Safely extract content from various document formats.
#     Handles Document objects, dicts, and strings.
#     """
#     try:
#         print(f"üìÑ [DOC] Extracting content from type: {type(doc)}")
        
#         # Case 1: Document object with page_content attribute
#         if hasattr(doc, "page_content"):
#             content = str(doc.page_content).strip()
#             print(f"‚úÖ [DOC] Extracted from page_content attribute: {content[:50]}...")
#             return content
        
#         # Case 2: Dictionary with page_content key
#         elif isinstance(doc, dict):
#             print(f"üîë [DOC] Dict keys: {list(doc.keys())}")
#             if "page_content" in doc:
#                 content = str(doc["page_content"]).strip()
#                 print(f"‚úÖ [DOC] Extracted from page_content key: {content[:50]}...")
#                 return content
#             elif "content" in doc:
#                 content = str(doc["content"]).strip()
#                 print(f"‚úÖ [DOC] Extracted from content key: {content[:50]}...")
#                 return content
#             else:
#                 content = str(doc).strip()
#                 print(f"‚ö†Ô∏è [DOC] No content key, using string repr: {content[:50]}...")
#                 return content
        
#         # Case 3: Plain string
#         elif isinstance(doc, str):
#             print(f"‚úÖ [DOC] Already a string: {doc[:50]}...")
#             return doc.strip()
        
#         # Case 4: Unknown type, convert to string
#         else:
#             content = str(doc).strip()
#             print(f"‚ö†Ô∏è [DOC] Unknown type, converting to string: {content[:50]}...")
#             logger.warning(f"Unknown document type: {type(doc)}")
#             return content
            
#     except Exception as e:
#         print(f"‚ùå [DOC ERROR] {e}")
#         logger.error(f"Error extracting content from doc: {e}")
#         return ""


# def serialize_docs(docs):
#     """
#     Convert documents to JSON-serializable format.
#     Returns a list of dicts with content and metadata.
#     """
#     print(f"üîÑ [SERIALIZE] Starting serialization of {len(docs)} docs")
#     serialized = []
    
#     for i, doc in enumerate(docs):
#         try:
#             print(f"üîÑ [SERIALIZE] Doc {i}: type={type(doc)}")
            
#             # Extract content
#             content = extract_content_from_doc(doc)
            
#             # Extract metadata if available
#             metadata = {}
#             if hasattr(doc, "metadata"):
#                 metadata = doc.metadata
#                 print(f"üìã [SERIALIZE] Doc {i}: Found metadata attribute")
#             elif isinstance(doc, dict) and "metadata" in doc:
#                 metadata = doc["metadata"]
#                 print(f"üìã [SERIALIZE] Doc {i}: Found metadata key")
            
#             result = {
#                 "content": content,
#                 "metadata": metadata
#             }
#             serialized.append(result)
#             print(f"‚úÖ [SERIALIZE] Doc {i}: Successfully serialized")
            
#         except Exception as e:
#             print(f"‚ùå [SERIALIZE ERROR] Doc {i}: {e}")
#             logger.error(f"Error serializing doc: {e}")
#             continue
    
#     print(f"‚úÖ [SERIALIZE] Completed: {len(serialized)} docs serialized")
#     return serialized


# def chat_with_memory_db(db: Session, user_id: UUID, question: str):
#     """Main Sharma Ji Bot Function (Tavily + Memory + Vector + LLM)"""
#     try:
#         print(f"\n{'='*60}")
#         print(f"üöÄ [START] New request")
#         print(f"üë§ [USER] ID: {user_id}")
#         print(f"‚ùì [QUESTION] {question}")
#         print(f"{'='*60}\n")
        
#         logger.info(f"Processing question: {question}")
#         current_info = get_current_datetime_info()

#         # Load chat memory
#         print("üíæ [MEMORY] Loading chat history...")
#         db_history = get_user_chat_memory(db, user_id)
#         print(f"üíæ [MEMORY] Found {len(db_history)} previous messages")
        
#         messages = []
#         for item in db_history:
#             messages.append(HumanMessage(content=item.question))
#             messages.append(AIMessage(content=item.answer))
#         print(f"üíæ [MEMORY] Built {len(messages)} message objects")

#         # Handle direct date/time queries
#         date_time_keywords = [
#             "aaj ki date", "today date", "aaj ka din", "current date",
#             "kya tarikh", "aaj kya date", "date kya hai", "time kya hai"
#         ]
#         print(f"üîç [CHECK] Checking for date/time keywords...")
#         if any(k in question.lower() for k in date_time_keywords):
#             print("‚úÖ [CHECK] Date/time keyword detected!")
#             answer = f"Aaj {current_info['day']} hai, date {current_info['date']} aur time {current_info['time']} hai."
#             save_chat_memory(db, ChatMemoryCreate(user_id=user_id, question=question, answer=answer))
#             print(f"üí¨ [ANSWER] {answer}")
#             return {"answer": answer, "context": [], "source_type": "system_time"}

#         # Tavily Search (for real-time info)
#         realtime_keywords = ["latest", "current", "news", "weather", "today", "2025", "score", "update", "ipl", "match", "win", "winner"]
#         print(f"üîç [CHECK] Checking for realtime keywords...")
#         needs_search = any(k in question.lower() for k in realtime_keywords)
#         print(f"üîç [CHECK] Needs search: {needs_search}")

#         if needs_search:
#             try:
#                 print(f"üåê [TAVILY] Search triggered for: {question}")
#                 logger.info(f"Tavily Search triggered for: {question}")
                
#                 tavily_results = tavily_client.search(query=question, max_results=5)
#                 print(f"üåê [TAVILY] Received {len(tavily_results.get('results', []))} results")

#                 formatted_results = "\n\n".join(
#                     [f"- {r.get('title', 'No Title')}: {r.get('url', 'No URL')}\n{r.get('content', 'No content')}" 
#                      for r in tavily_results.get("results", [])]
#                 )
#                 print(f"üåê [TAVILY] Formatted results length: {len(formatted_results)} chars")

#                 print("ü§ñ [LLM] Generating answer from Tavily results...")
#                 chain = search_prompt | llm | StrOutputParser()
#                 search_answer = chain.invoke({
#                     "question": question,
#                     "search_results": formatted_results,
#                     "current_date": current_info['date']
#                 })
#                 print(f"‚úÖ [LLM] Answer generated: {search_answer[:100]}...")

#                 print("üíæ [MEMORY] Saving to database...")
#                 save_chat_memory(db, ChatMemoryCreate(user_id=user_id, question=question, answer=search_answer))
#                 print("‚úÖ [MEMORY] Saved successfully")
                
#                 # Serialize Tavily results to ensure JSON compatibility
#                 print("üîÑ [TAVILY] Serializing Tavily results...")
#                 serialized_tavily = []
#                 for r in tavily_results.get("results", []):
#                     try:
#                         serialized_tavily.append({
#                             "title": str(r.get("title", "")),
#                             "url": str(r.get("url", "")),
#                             "content": str(r.get("content", "")),
#                             "score": float(r.get("score", 0.0)) if r.get("score") else 0.0
#                         })
#                     except Exception as ser_err:
#                         print(f"‚ö†Ô∏è [TAVILY] Failed to serialize result: {ser_err}")
#                         continue
                
#                 print(f"‚úÖ [TAVILY] Serialized {len(serialized_tavily)} results")
                
#                 result = {
#                     "answer": search_answer,
#                     "context": serialized_tavily,
#                     "source_type": "tavily_search"
#                 }
#                 print(f"‚úÖ [RETURN] Returning Tavily result with {len(serialized_tavily)} context items")
#                 return result

#             except Exception as tavily_error:
#                 print(f"‚ùå [TAVILY ERROR] {tavily_error}")
#                 print(f"‚ùå [TAVILY TRACE] {traceback.format_exc()}")
#                 logger.error(f"Tavily Search Error: {tavily_error}")
#                 logger.debug(traceback.format_exc())
#                 print("‚ö†Ô∏è [FALLBACK] Continuing to vector fallback...")

#         # Vector DB Retrieval (fallback)
#         print("üîç [VECTOR] Starting vector retrieval...")
#         context = "No relevant context found."
#         docs = []
#         serialized_context = []

#         try:
#             print(f"üîç [VECTOR] Invoking retriever with question: {question}")
#             docs = retriever.invoke(question)
#             print(f"‚úÖ [VECTOR] Retriever returned {len(docs)} items")
            
#             if docs:
#                 print(f"üîç [VECTOR] First doc type: {type(docs[0])}")
#                 print(f"üîç [VECTOR] First doc content: {docs[0]}")
                
#                 # Check if it's a dict with direct access
#                 if isinstance(docs[0], dict):
#                     print(f"üîç [VECTOR] First doc keys: {list(docs[0].keys())}")
#             else:
#                 print("‚ö†Ô∏è [VECTOR] No docs returned!")

#             context_parts = []
#             for i, doc in enumerate(docs):
#                 try:
#                     print(f"üîç [VECTOR] Processing doc {i}... Type: {type(doc)}")
#                     content = extract_content_from_doc(doc)
                    
#                     if content:
#                         context_parts.append(content)
#                         print(f"‚úÖ [VECTOR] Doc {i} added: {content[:100]}...")
#                     else:
#                         print(f"‚ö†Ô∏è [VECTOR] Doc {i} returned empty content")
                        
#                 except Exception as doc_error:
#                     print(f"‚ùå [VECTOR] Error processing doc {i}: {doc_error}")
#                     print(f"‚ùå [VECTOR] Doc {i} trace: {traceback.format_exc()}")
#                     continue

#             context = "\n\n".join(context_parts) if context_parts else "No relevant context found."
#             print(f"üß© [VECTOR] Built context from {len(context_parts)} chunks")
#             print(f"üß© [VECTOR] Total context length: {len(context)} chars")
            
#             # Serialize docs for JSON response
#             print("üîÑ [VECTOR] Starting serialization...")
#             try:
#                 serialized_context = serialize_docs(docs)
#                 print(f"‚úÖ [VECTOR] Serialization complete: {len(serialized_context)} items")
#             except Exception as serialize_error:
#                 print(f"‚ùå [VECTOR] Serialization failed: {serialize_error}")
#                 print(f"‚ùå [VECTOR] Serialize trace: {traceback.format_exc()}")
#                 serialized_context = []

#         except Exception as retriever_error:
#             print(f"‚ùå [VECTOR ERROR] {retriever_error}")
#             print(f"‚ùå [VECTOR TRACE] {traceback.format_exc()}")
#             logger.error(f"‚ùå Retriever Error: {retriever_error}")
#             logger.debug(traceback.format_exc())
#             context = "Context retrieval failed."
#             serialized_context = []

#         # Generate final answer
#         try:
#             print("ü§ñ [LLM] Generating final answer...")
#             print(f"ü§ñ [LLM] Context length: {len(context)}")
#             print(f"ü§ñ [LLM] Chat history length: {len(messages)}")
            
#             chain = prompt | llm | StrOutputParser()
#             answer = chain.invoke({
#                 "input": question,
#                 "chat_history": messages,
#                 "context": context
#             })
#             print(f"‚úÖ [LLM] Answer generated: {answer[:100]}...")

#             print("üíæ [MEMORY] Saving to database...")
#             save_chat_memory(db, ChatMemoryCreate(user_id=user_id, question=question, answer=answer))
#             print("‚úÖ [MEMORY] Saved successfully")
            
#             result = {
#                 "answer": answer,
#                 "context": serialized_context,
#                 "source_type": "vectorstore"
#             }
#             print(f"‚úÖ [RETURN] Returning vectorstore result")
#             print(f"‚úÖ [RETURN] Context items: {len(serialized_context)}")
#             return result

#         except Exception as llm_error:
#             print(f"‚ùå [LLM ERROR] {llm_error}")
#             print(f"‚ùå [LLM TRACE] {traceback.format_exc()}")
#             logger.error(f"LLM generation error: {llm_error}")
#             answer = "Sorry, answer generate nahi kar paya abhi. Thodi der baad try karo."
#             save_chat_memory(db, ChatMemoryCreate(user_id=user_id, question=question, answer=answer))
#             return {
#                 "answer": answer,
#                 "context": [],
#                 "source_type": "error"
#             }

#     except Exception as e:
#         print(f"‚ùå [CRITICAL ERROR] {e}")
#         print(f"‚ùå [CRITICAL TRACE] {traceback.format_exc()}")
#         logger.error(f"Critical Error: {e}")
#         logger.debug(traceback.format_exc())
#         return {
#             "answer": f"Sorry, ek technical issue hua: {e}",
#             "context": [],
#             "source_type": "error"
#         }





from langchain_core.tools import Tool
from sqlalchemy.orm import Session
from app.repositories.chat_memory_repository import get_user_chat_memory, save_chat_memory
from app.schemas.chat_memory_schema import ChatMemoryCreate
from langchain_core.messages import HumanMessage, AIMessage
from app.core.config import settings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from tavily import TavilyClient
from uuid import UUID
import traceback
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

# Initialize LLM, Vectorstore, and Tavily Search
try:
    print("üîß [INIT] Starting initialization...")
    embeddings = OpenAIEmbeddings(api_key=settings.OPENAI_API_KEY)
    CHROMA_PATH = "app/db/chroma_storage_new"
    print(f"üìÅ [INIT] Chroma path: {CHROMA_PATH}")

    vectorstore = Chroma(
        persist_directory=CHROMA_PATH,
        embedding_function=embeddings
    )
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    print("‚úÖ [INIT] Vectorstore initialized")

    llm = ChatOpenAI(
        model=settings.OPENAI_MODEL,
        temperature=0.3,
        api_key=settings.OPENAI_API_KEY
    )
    print(f"‚úÖ [INIT] LLM initialized: {settings.OPENAI_MODEL}")

    tavily_client = TavilyClient(api_key=settings.TAVILY_API_KEY)
    print("‚úÖ [INIT] Tavily Search initialized successfully")
    logger.info("Tavily Search initialized successfully")

except Exception as init_error:
    print(f"‚ùå [INIT ERROR] {init_error}")
    logger.error(f"Initialization error: {init_error}")
    logger.debug(traceback.format_exc())
    raise

# System Prompt (Hinglish + Personality)
prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        f"""
        Tum {settings.BOT_NAME} ho ‚Äî ek intelligent, polite aur thoda witty AI assistant 
        jise {settings.CREATOR_NAME} ne banaya hai.

        Rules:
        - Mix Hindi + English
        - Professional yet friendly
        - Apna character hamesha maintain karo
        - Agar user pooche "tum kaun ho", to confidently intro do
        """
    ),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "Question: {input}\n\nContext:\n{context}")
])

# Tavily Search Prompt
search_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        f"""
        Tum {settings.BOT_NAME} ho ‚Äî ek smart AI assistant.
        Tumhe Tavily search se realtime data milta hai.

        Current date: {{current_date}}.
        Jo data mila hai, use concise aur apne tone me explain karo.
        Agar data outdated ho to user ko warn kar dena.
        """
    ),
    ("human", "Question: {question}\n\nTavily Results:\n{search_results}\n\nAnswer naturally:")
])


def get_current_datetime_info():
    """Get current date/time details"""
    now = datetime.now()
    info = {
        "date": now.strftime("%d %B %Y"),
        "time": now.strftime("%I:%M %p"),
        "day": now.strftime("%A"),
    }
    print(f"üìÖ [TIME] Current info: {info}")
    return info


def extract_content_from_doc(doc):
    """
    Safely extract content from various document formats.
    Handles Document objects, dicts, and strings.
    """
    try:
        print(f"üìÑ [DOC] Extracting content from type: {type(doc)}")
        
        # Case 1: Document object with page_content attribute
        if hasattr(doc, "page_content"):
            content = str(doc.page_content).strip()
            print(f"‚úÖ [DOC] Extracted from page_content attribute: {content[:50]}...")
            return content
        
        # Case 2: Dictionary with page_content key
        elif isinstance(doc, dict):
            print(f"üîë [DOC] Dict keys: {list(doc.keys())}")
            if "page_content" in doc:
                content = str(doc["page_content"]).strip()
                print(f"‚úÖ [DOC] Extracted from page_content key: {content[:50]}...")
                return content
            elif "content" in doc:
                content = str(doc["content"]).strip()
                print(f"‚úÖ [DOC] Extracted from content key: {content[:50]}...")
                return content
            else:
                content = str(doc).strip()
                print(f"‚ö†Ô∏è [DOC] No content key, using string repr: {content[:50]}...")
                return content
        
        # Case 3: Plain string
        elif isinstance(doc, str):
            print(f"‚úÖ [DOC] Already a string: {doc[:50]}...")
            return doc.strip()
        
        # Case 4: Unknown type, convert to string
        else:
            content = str(doc).strip()
            print(f"‚ö†Ô∏è [DOC] Unknown type, converting to string: {content[:50]}...")
            logger.warning(f"Unknown document type: {type(doc)}")
            return content
            
    except Exception as e:
        print(f"‚ùå [DOC ERROR] {e}")
        logger.error(f"Error extracting content from doc: {e}")
        return ""


def serialize_docs(docs):
    """
    Convert documents to JSON-serializable format.
    Returns a list of dicts with content and metadata.
    """
    print(f"üîÑ [SERIALIZE] Starting serialization of {len(docs)} docs")
    serialized = []
    
    for i, doc in enumerate(docs):
        try:
            print(f"üîÑ [SERIALIZE] Doc {i}: type={type(doc)}")
            
            # Extract content
            content = extract_content_from_doc(doc)
            
            # Extract metadata if available
            metadata = {}
            if hasattr(doc, "metadata"):
                metadata = doc.metadata
                print(f"üìã [SERIALIZE] Doc {i}: Found metadata attribute")
            elif isinstance(doc, dict) and "metadata" in doc:
                metadata = doc["metadata"]
                print(f"üìã [SERIALIZE] Doc {i}: Found metadata key")
            
            result = {
                "content": content,
                "metadata": metadata
            }
            serialized.append(result)
            print(f"‚úÖ [SERIALIZE] Doc {i}: Successfully serialized")
            
        except Exception as e:
            print(f"‚ùå [SERIALIZE ERROR] Doc {i}: {e}")
            logger.error(f"Error serializing doc: {e}")
            continue
    
    print(f"‚úÖ [SERIALIZE] Completed: {len(serialized)} docs serialized")
    return serialized


def detect_realtime_intent(question: str) -> bool:
    """Smart detection for realtime or dynamic queries."""
    q = question.lower().strip()

    realtime_keywords = [
        "latest", "current", "today", "now", "live", "update", "trending", "recent", "breaking", "abhi",
        "aaj", "abhi ka", "kal ka", "score", "match", "price", "rate", "value", "weather", "temperature",
        "winner", "ipl", "news", "gold", "silver", "bitcoin", "stock", "sensex", "bazar", "market"
    ]

    finance_words = ["price", "rate", "value", "gold", "silver", "usd", "bitcoin", "crypto", "share", "stock"]
    event_words = ["ipl", "match", "score", "winner", "tournament", "game"]
    news_words = ["news", "update", "today", "breaking", "headline"]
    weather_words = ["weather", "temperature", "climate", "rain", "humidity"]

    if any(word in q for word in ["aaj", "abhi", "today", "current", "latest", "now"]) and \
       any(w in q for w in (finance_words + event_words + news_words + weather_words)):
        return True

    if any(str(y) in q for y in range(2023, 2031)):
        return True

    if any(k in q for k in realtime_keywords):
        return True

    return False




def chat_with_memory_db(db: Session, user_id: UUID, question: str):
    """Main Sharma Ji Bot Function (Tavily + Memory + Vector + LLM)"""
    try:
        print(f"\n{'='*60}")
        print(f"üöÄ [START] New request")
        print(f"üë§ [USER] ID: {user_id}")
        print(f"‚ùì [QUESTION] {question}")
        print(f"{'='*60}\n")
        
        logger.info(f"Processing question: {question}")
        current_info = get_current_datetime_info()

        # Load chat memory
        print("üíæ [MEMORY] Loading chat history...")
        db_history = get_user_chat_memory(db, user_id)
        print(f"üíæ [MEMORY] Found {len(db_history)} previous messages")
        
        messages = []
        for item in db_history:
            messages.append(HumanMessage(content=item.question))
            messages.append(AIMessage(content=item.answer))
        print(f"üíæ [MEMORY] Built {len(messages)} message objects")

        # Handle direct date/time queries
        date_time_keywords = [
            "aaj ki date", "today date", "aaj ka din", "current date",
            "kya tarikh", "aaj kya date", "date kya hai", "time kya hai"
        ]
        print(f"üîç [CHECK] Checking for date/time keywords...")
        if any(k in question.lower() for k in date_time_keywords):
            print("‚úÖ [CHECK] Date/time keyword detected!")
            answer = f"Aaj {current_info['day']} hai, date {current_info['date']} aur time {current_info['time']} hai."
            save_chat_memory(db, ChatMemoryCreate(user_id=user_id, question=question, answer=answer))
            print(f"üí¨ [ANSWER] {answer}")
            return {"answer": answer, "context": [], "source_type": "system_time"}

        # Tavily Search (for real-time info)
        print("üîç [CHECK] Detecting if question needs realtime search...")
        needs_search = detect_realtime_intent(question)
        print(f"üîç [CHECK] Realtime intent detected: {needs_search}")



        if needs_search:
            try:
                print(f"üåê [TAVILY] Search triggered for: {question}")
                logger.info(f"Tavily Search triggered for: {question}")
                
                tavily_results = tavily_client.search(query=question, max_results=5)
                print(f"üåê [TAVILY] Received {len(tavily_results.get('results', []))} results")

                formatted_results = "\n\n".join(
                    [f"- {r.get('title', 'No Title')}: {r.get('url', 'No URL')}\n{r.get('content', 'No content')}" 
                     for r in tavily_results.get("results", [])]
                )
                print(f"üåê [TAVILY] Formatted results length: {len(formatted_results)} chars")

                print("ü§ñ [LLM] Generating answer from Tavily results...")
                chain = search_prompt | llm | StrOutputParser()
                search_answer = chain.invoke({
                    "question": question,
                    "search_results": formatted_results,
                    "current_date": current_info['date']
                })
                print(f"‚úÖ [LLM] Answer generated: {search_answer[:100]}...")

                print("üíæ [MEMORY] Saving to database...")
                save_chat_memory(db, ChatMemoryCreate(user_id=user_id, question=question, answer=search_answer))
                print("‚úÖ [MEMORY] Saved successfully")
                
                # Serialize Tavily results to ensure JSON compatibility
                print("üîÑ [TAVILY] Serializing Tavily results...")
                serialized_tavily = []
                for r in tavily_results.get("results", []):
                    try:
                        serialized_tavily.append({
                            "title": str(r.get("title", "")),
                            "url": str(r.get("url", "")),
                            "content": str(r.get("content", "")),
                            "score": float(r.get("score", 0.0)) if r.get("score") else 0.0
                        })
                    except Exception as ser_err:
                        print(f"‚ö†Ô∏è [TAVILY] Failed to serialize result: {ser_err}")
                        continue
                
                print(f"‚úÖ [TAVILY] Serialized {len(serialized_tavily)} results")
                
                result = {
                    "answer": search_answer,
                    "context": serialized_tavily,
                    "source_type": "tavily_search"
                }
                print(f"‚úÖ [RETURN] Returning Tavily result with {len(serialized_tavily)} context items")
                return result

            except Exception as tavily_error:
                print(f"‚ùå [TAVILY ERROR] {tavily_error}")
                print(f"‚ùå [TAVILY TRACE] {traceback.format_exc()}")
                logger.error(f"Tavily Search Error: {tavily_error}")
                logger.debug(traceback.format_exc())
                print("‚ö†Ô∏è [FALLBACK] Continuing to vector fallback...")

        # Vector DB Retrieval (fallback)
        print("üîç [VECTOR] Starting vector retrieval...")
        context = "No relevant context found."
        docs = []
        serialized_context = []

        try:
            print(f"üîç [VECTOR] Invoking retriever with question: {question}")
            docs = retriever.invoke(question)
            print(f"‚úÖ [VECTOR] Retriever returned {len(docs)} items")
            
            if docs:
                print(f"üîç [VECTOR] First doc type: {type(docs[0])}")
                print(f"üîç [VECTOR] First doc content: {docs[0]}")
                
                # Check if it's a dict with direct access
                if isinstance(docs[0], dict):
                    print(f"üîç [VECTOR] First doc keys: {list(docs[0].keys())}")
            else:
                print("‚ö†Ô∏è [VECTOR] No docs returned!")

            context_parts = []
            for i, doc in enumerate(docs):
                try:
                    print(f"üîç [VECTOR] Processing doc {i}... Type: {type(doc)}")
                    content = extract_content_from_doc(doc)
                    
                    if content:
                        context_parts.append(content)
                        print(f"‚úÖ [VECTOR] Doc {i} added: {content[:100]}...")
                    else:
                        print(f"‚ö†Ô∏è [VECTOR] Doc {i} returned empty content")
                        
                except Exception as doc_error:
                    print(f"‚ùå [VECTOR] Error processing doc {i}: {doc_error}")
                    print(f"‚ùå [VECTOR] Doc {i} trace: {traceback.format_exc()}")
                    continue

            context = "\n\n".join(context_parts) if context_parts else "No relevant context found."
            print(f"üß© [VECTOR] Built context from {len(context_parts)} chunks")
            print(f"üß© [VECTOR] Total context length: {len(context)} chars")
            
            # Serialize docs for JSON response
            print("üîÑ [VECTOR] Starting serialization...")
            try:
                serialized_context = serialize_docs(docs)
                print(f"‚úÖ [VECTOR] Serialization complete: {len(serialized_context)} items")
            except Exception as serialize_error:
                print(f"‚ùå [VECTOR] Serialization failed: {serialize_error}")
                print(f"‚ùå [VECTOR] Serialize trace: {traceback.format_exc()}")
                serialized_context = []

        except Exception as retriever_error:
            print(f"‚ùå [VECTOR ERROR] {retriever_error}")
            print(f"‚ùå [VECTOR TRACE] {traceback.format_exc()}")
            logger.error(f"‚ùå Retriever Error: {retriever_error}")
            logger.debug(traceback.format_exc())
            context = "Context retrieval failed."
            serialized_context = []

        # Generate final answer
        try:
            print("ü§ñ [LLM] Generating final answer...")
            print(f"ü§ñ [LLM] Context length: {len(context)}")
            print(f"ü§ñ [LLM] Chat history length: {len(messages)}")
            
            chain = prompt | llm | StrOutputParser()
            answer = chain.invoke({
                "input": question,
                "chat_history": messages,
                "context": context
            })
            print(f"‚úÖ [LLM] Answer generated: {answer[:100]}...")

            print("üíæ [MEMORY] Saving to database...")
            save_chat_memory(db, ChatMemoryCreate(user_id=user_id, question=question, answer=answer))
            print("‚úÖ [MEMORY] Saved successfully")
            
            result = {
                "answer": answer,
                "context": serialized_context,
                "source_type": "vectorstore"
            }
            print(f"‚úÖ [RETURN] Returning vectorstore result")
            print(f"‚úÖ [RETURN] Context items: {len(serialized_context)}")
            return result

        except Exception as llm_error:
            print(f"‚ùå [LLM ERROR] {llm_error}")
            print(f"‚ùå [LLM TRACE] {traceback.format_exc()}")
            logger.error(f"LLM generation error: {llm_error}")
            answer = "Sorry, answer generate nahi kar paya abhi. Thodi der baad try karo."
            save_chat_memory(db, ChatMemoryCreate(user_id=user_id, question=question, answer=answer))
            return {
                "answer": answer,
                "context": [],
                "source_type": "error"
            }

    except Exception as e:
        print(f"‚ùå [CRITICAL ERROR] {e}")
        print(f"‚ùå [CRITICAL TRACE] {traceback.format_exc()}")
        logger.error(f"Critical Error: {e}")
        logger.debug(traceback.format_exc())
        return {
            "answer": f"Sorry, ek technical issue hua: {e}",
            "context": [],
            "source_type": "error"
        }